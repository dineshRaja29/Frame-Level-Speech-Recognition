{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshRaja29/Frame-Level-Speech-Recognition/blob/main/EXPERIMENT_II_WITHOUT_OUTPUT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'>Frame-Level Speech Recognition"
      ],
      "metadata": {
        "id": "F9ERgBpbcMmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'> EXPERIMENT\n",
        "\n",
        "* Build LA-DNN\n",
        "    * A powerful model for speech recognition and good candidate to replace FF-DNN\n",
        "    * Helps to remove the vanshing gradient problems in deep network\n",
        "    * https://ieeexplore.ieee.org/document/7472646\n",
        "* No Augmentation\n",
        "* No hyper-parameter tuning however used our past knowledge to set hyper-paramters\n",
        "* Advanced weight initialization, Label smoothing in loss function, gradient clipping and scheduler are used."
      ],
      "metadata": {
        "id": "sl8pUcOV2WiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary --quiet"
      ],
      "metadata": {
        "id": "rwYu9sSUnSho",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-02T06:40:53.370881Z",
          "iopub.execute_input": "2025-06-02T06:40:53.371165Z",
          "iopub.status.idle": "2025-06-02T06:40:56.373997Z",
          "shell.execute_reply.started": "2025-06-02T06:40:53.371143Z",
          "shell.execute_reply": "2025-06-02T06:40:56.373203Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import torchsummary\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "id": "qI4qfx7tiBZt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:49:16.811881Z",
          "iopub.execute_input": "2025-06-14T05:49:16.812231Z",
          "iopub.status.idle": "2025-06-14T05:49:16.817707Z",
          "shell.execute_reply.started": "2025-06-14T05:49:16.81221Z",
          "shell.execute_reply": "2025-06-14T05:49:16.816818Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8yBgXjKV1O0Z",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir -p /root/.kaggle"
      ],
      "metadata": {
        "id": "Q90JyTKuraM1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:49:16.821689Z",
          "iopub.execute_input": "2025-06-14T05:49:16.821939Z",
          "iopub.status.idle": "2025-06-14T05:49:23.33869Z",
          "shell.execute_reply.started": "2025-06-14T05:49:16.821916Z",
          "shell.execute_reply": "2025-06-14T05:49:23.337612Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"dineshbuswala\",\"key\":\"a67fefaecacb98180c9d56e1b00b37de\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TPBUd7Cnl-Rx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:49:27.435586Z",
          "iopub.execute_input": "2025-06-14T05:49:27.436294Z",
          "iopub.status.idle": "2025-06-14T05:49:27.569906Z",
          "shell.execute_reply.started": "2025-06-14T05:49:27.436258Z",
          "shell.execute_reply": "2025-06-14T05:49:27.568695Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# commands to download data from kaggle\n",
        "\n",
        "!kaggle competitions download -c 11785-hw1p2-f23 --force\n",
        "!mkdir -p '/content/data'\n",
        "\n",
        "# !unzip -qo /content/11785-hw1p2-f23.zip -d '/content/data'"
      ],
      "metadata": {
        "id": "if2Somqfbje1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:49:31.160591Z",
          "iopub.execute_input": "2025-06-14T05:49:31.161294Z",
          "iopub.status.idle": "2025-06-14T05:49:57.544197Z",
          "shell.execute_reply.started": "2025-06-14T05:49:31.161263Z",
          "shell.execute_reply": "2025-06-14T05:49:57.54329Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo /kaggle/working/11785-hw1p2-f23.zip -d '/content/data'"
      ],
      "metadata": {
        "trusted": true,
        "id": "2ghCRGEaSN2R",
        "execution": {
          "iopub.status.busy": "2025-06-14T05:49:57.546071Z",
          "iopub.execute_input": "2025-06-14T05:49:57.546378Z",
          "iopub.status.idle": "2025-06-14T05:50:55.418722Z",
          "shell.execute_reply.started": "2025-06-14T05:49:57.546351Z",
          "shell.execute_reply": "2025-06-14T05:50:55.417507Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ! rm -r /content/data/11-785-f23-hw1p2/test-clean\n",
        "# ! rm -r /content/11785-hw1p2-f23.zip"
      ],
      "metadata": {
        "id": "90HjJ5xEegS3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /content/data/11-785-f23-hw1p2/train-clean-100/mfcc/ | wc -l"
      ],
      "metadata": {
        "id": "32hJUYftmPDf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:50:55.420449Z",
          "iopub.execute_input": "2025-06-14T05:50:55.420739Z",
          "iopub.status.idle": "2025-06-14T05:50:55.659048Z",
          "shell.execute_reply.started": "2025-06-14T05:50:55.42071Z",
          "shell.execute_reply": "2025-06-14T05:50:55.658142Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "### configuration variables\n",
        "EPOCHS                          = 4\n",
        "BATCH_SIZE                      = 2048 * 2\n",
        "LEFT_CONTEXT                    = 7\n",
        "RIGHT_CONTEXT       \t        = 7\n",
        "INITIAL_LEARNING_RATE           = 1e-3\n",
        "L2_PENALTY                      = 1e-5\n",
        "STEP_SIZE                       = 2\n",
        "GAMMA                           = 0.1\n",
        "BASE_DIRECTORY                  = '/content/data/11-785-f23-hw1p2/'\n",
        "TRAINING_DATA                   = BASE_DIRECTORY + 'train-clean-100'\n",
        "EVALUATION_DATA                 = BASE_DIRECTORY + 'dev-clean'\n",
        "PHONEMES                        = ['[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "                                    'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "                                    'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "                                    'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "                                    'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "                                    'V',     'W',     'Y',     'Z',     'ZH'] #,    '[SOS]', '[EOS]']\n",
        "PHONEMES_TO_INDEX                = {phoneme: idx for idx, phoneme in enumerate(PHONEMES)}\n",
        "NUMBER_OF_NEURONS                = [2048, 2048, 1024, 1024, 512, 256, 256]\n",
        "MODEL_DIR                        = \"/content\"\n",
        "CLIP_VALUE                       = 1.0\n",
        "LABEL_SMOOTHING                  = 0.01"
      ],
      "metadata": {
        "id": "Kc4IbXuwx0Tf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:51:53.601383Z",
          "iopub.execute_input": "2025-06-14T05:51:53.601714Z",
          "iopub.status.idle": "2025-06-14T05:51:53.607793Z",
          "shell.execute_reply.started": "2025-06-14T05:51:53.601693Z",
          "shell.execute_reply": "2025-06-14T05:51:53.607045Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, phonemes = PHONEMES_TO_INDEX,\n",
        "                 left_context = LEFT_CONTEXT,\n",
        "                 right_context = RIGHT_CONTEXT):\n",
        "\n",
        "        self.left_context = left_context\n",
        "        self.right_context = right_context\n",
        "        self.phonemes_mapping = phonemes\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, 'mfcc')\n",
        "        self.transcript_dir = os.path.join(root, 'transcript')\n",
        "\n",
        "        # List and sort mfcc and transcript files\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "        transcript_names = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(mfcc_names) == len(transcript_names), \"Mismatch in number of MFCC and transcript files\"\n",
        "\n",
        "        total_frames = 0\n",
        "\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc_path = os.path.join(self.mfcc_dir, mfcc_names[i])\n",
        "\n",
        "            mfcc = np.load(mfcc_path,\n",
        "                           allow_pickle = False,\n",
        "                           mmap_mode='r')\n",
        "            total_frames += mfcc.shape[0]\n",
        "            del mfcc\n",
        "\n",
        "        sample_mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[0]), allow_pickle=False)\n",
        "        self.mfcc_dim = sample_mfcc.shape[1]\n",
        "        ### Right Padding is added here automatically\n",
        "        self.mfccs = np.zeros((total_frames + right_context, self.mfcc_dim))\n",
        "        self.transcripts = [None] * (total_frames + right_context)\n",
        "        ### Release memory\n",
        "        del sample_mfcc, total_frames\n",
        "        gc.collect()\n",
        "\n",
        "        index = 0\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]),\n",
        "                           allow_pickle = False,\n",
        "                           mmap_mode = 'r')\n",
        "\n",
        "            mfcc = (mfcc - mfcc.mean(axis = 1, keepdims = True)) / (mfcc.std(axis = 1, keepdims = True) + 1e-5)\n",
        "\n",
        "            transcript = np.load(os.path.join(self.transcript_dir, transcript_names[i]),\n",
        "                                 allow_pickle = False,\n",
        "                                 mmap_mode='r')[1:-1]\n",
        "\n",
        "            self.mfccs[index: index + mfcc.shape[0]] = mfcc\n",
        "            self.transcripts[index: index + len(transcript)] = transcript\n",
        "            index += mfcc.shape[0]\n",
        "            del mfcc, transcript\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                gc.collect() ### Release memory\n",
        "\n",
        "        # Save original dataset length (before adding right padding)\n",
        "        self.length = len(self.mfccs) - right_context\n",
        "\n",
        "        # Map transcript phonemes to indices\n",
        "        self.transcripts = [self.phonemes_mapping.get(p, -1) for p in self.transcripts]\n",
        "\n",
        "        ### Release memory\n",
        "        del mfcc_names, transcript_names\n",
        "        gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        if ind < self.left_context:  # index is less than left context\n",
        "            # zeros need to prepend with it\n",
        "            padding = []\n",
        "            for _ in range(self.left_context - ind):\n",
        "                padding.append(np.zeros((1, self.mfcc_dim)))\n",
        "\n",
        "            for i in range(ind):\n",
        "                padding.append(self.mfccs[i][None, :])\n",
        "\n",
        "            # Include current and right context frames\n",
        "            current = self.mfccs[ind][None, :]  # Ensure shape (1, 28)\n",
        "            right_context = [self.mfccs[i][None, :] for i in range(ind + 1, ind + 1 + self.right_context)]\n",
        "            frames = np.concatenate(padding + [current] + right_context, axis=0)\n",
        "\n",
        "        else:  # when index is greater than or equal to left context\n",
        "            left_context_frames = self.mfccs[ind - self.left_context: ind]\n",
        "            current = self.mfccs[ind][None, :]  # Ensure shape (1, 28)\n",
        "            right_context_frames = self.mfccs[ind + 1: ind + 1 + self.right_context]\n",
        "\n",
        "            frames = np.concatenate([left_context_frames, current, right_context_frames], axis=0)\n",
        "\n",
        "        frames = frames.flatten()  # Flatten to get 1D data\n",
        "        frames = torch.FloatTensor(frames)  # Convert to tensor\n",
        "        phonemes = torch.tensor(self.transcripts[ind], dtype=torch.long)  # Convert label to tensor\n",
        "\n",
        "        return frames, phonemes\n"
      ],
      "metadata": {
        "id": "gUN5oYyqq8ji",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:51:57.536354Z",
          "iopub.execute_input": "2025-06-14T05:51:57.537038Z",
          "iopub.status.idle": "2025-06-14T05:51:57.552285Z",
          "shell.execute_reply.started": "2025-06-14T05:51:57.537014Z",
          "shell.execute_reply": "2025-06-14T05:51:57.551646Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object using the AudioDataset class for the training data\n",
        "train_data = AudioDataset(TRAINING_DATA)\n",
        "\n",
        "# Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = AudioDataset(EVALUATION_DATA)\n"
      ],
      "metadata": {
        "id": "7xi7V8x8W9z4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:52:03.830442Z",
          "iopub.execute_input": "2025-06-14T05:52:03.830946Z",
          "iopub.status.idle": "2025-06-14T05:53:30.584206Z",
          "shell.execute_reply.started": "2025-06-14T05:52:03.830924Z",
          "shell.execute_reply": "2025-06-14T05:53:30.58354Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ],
      "metadata": {
        "id": "4mzoYfTKu14s",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:54:18.031479Z",
          "iopub.execute_input": "2025-06-14T05:54:18.032033Z",
          "iopub.status.idle": "2025-06-14T05:54:18.036965Z",
          "shell.execute_reply.started": "2025-06-14T05:54:18.032012Z",
          "shell.execute_reply": "2025-06-14T05:54:18.036228Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing code to check if data loaders are working as expecting\n",
        "# total_batches = len(train_loader)\n",
        "\n",
        "# for i, (frames, phoneme) in enumerate(train_loader):\n",
        "#     if i < 10 or i >= total_batches - 10:\n",
        "#         print(f\"Batch {i + 1}/{total_batches}\")\n",
        "#         print(\"Frames:\", frames)\n",
        "#         print(\"Phoneme:\", phoneme)\n",
        "#         print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "n-GV3UvgLSoF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class LADNNLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank = -1):\n",
        "        super(LADNNLayer, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        if rank == -1:\n",
        "            self.intermediate = in_features // 2\n",
        "        else:\n",
        "            self.intermediate = rank\n",
        "        self.U = torch.nn.Linear(self.in_features, self.intermediate)\n",
        "        self.V = torch.nn.Linear(self.intermediate, self.out_features)\n",
        "        self.T = torch.nn.Linear(self.in_features, self.out_features)\n",
        "\n",
        "        # Apply weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        print('Initialization of weights using Kaiming')\n",
        "        for m in [self.U] + [self.V] + [self.T]:\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                # Kaiming initialization for weights\n",
        "                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        t_x = self.T(x)\n",
        "        u_x = self.U(x)\n",
        "        v_x = self.V(self.activation(u_x))\n",
        "        return v_x + t_x\n",
        "\n",
        "\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Neurons in each layer: input -> hidden(s) -> output\n",
        "        self.neurons = [input_size] + NUMBER_OF_NEURONS + [len(PHONEMES)]\n",
        "\n",
        "        layers = []\n",
        "        for in_features, out_features in zip(self.neurons[:-2], self.neurons[1:-1]):\n",
        "            layers.append(LADNNLayer(in_features, out_features))\n",
        "\n",
        "        # Final layer\n",
        "        layers.append(LADNNLayer(self.neurons[-2], self.neurons[-1]))\n",
        "\n",
        "        # Combine all into a sequential model\n",
        "        self.model = torch.nn.Sequential(*layers)\n",
        "\n",
        "        # Apply weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        print('Initialization of weights using Kaiming')\n",
        "        for m in self.model:\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                # Kaiming initialization for weights\n",
        "                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "OvcpontXQq9j",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:54:22.940328Z",
          "iopub.execute_input": "2025-06-14T05:54:22.940929Z",
          "iopub.status.idle": "2025-06-14T05:54:22.949986Z",
          "shell.execute_reply.started": "2025-06-14T05:54:22.940904Z",
          "shell.execute_reply": "2025-06-14T05:54:22.949158Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = (LEFT_CONTEXT + RIGHT_CONTEXT + 1) * 28\n",
        "model = Network(INPUT_SIZE).to(device)\n",
        "# Pass the input size as a tuple, without the batch dimension\n",
        "torchsummary.summary(model, (INPUT_SIZE,))"
      ],
      "metadata": {
        "id": "d8HWyYl82Pnc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:54:29.815508Z",
          "iopub.execute_input": "2025-06-14T05:54:29.815794Z",
          "iopub.status.idle": "2025-06-14T05:54:30.836902Z",
          "shell.execute_reply.started": "2025-06-14T05:54:29.815772Z",
          "shell.execute_reply": "2025-06-14T05:54:30.836168Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing = LABEL_SMOOTHING) # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr = INITIAL_LEARNING_RATE,\n",
        "                             weight_decay = L2_PENALTY) #Defining Optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size = STEP_SIZE, gamma = GAMMA\n",
        ")\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
      ],
      "metadata": {
        "id": "UROGEVJevKD-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:55:29.136312Z",
          "iopub.execute_input": "2025-06-14T05:55:29.136634Z",
          "iopub.status.idle": "2025-06-14T05:55:33.057785Z",
          "shell.execute_reply.started": "2025-06-14T05:55:29.13661Z",
          "shell.execute_reply": "2025-06-14T05:55:33.05714Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XblOHEVtKab2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:55:36.1709Z",
          "iopub.execute_input": "2025-06-14T05:55:36.171363Z",
          "iopub.status.idle": "2025-06-14T05:55:36.468971Z",
          "shell.execute_reply.started": "2025-06-14T05:55:36.17134Z",
          "shell.execute_reply": "2025-06-14T05:55:36.468358Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        logits  = model(frames)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss    = criterion(logits, phonemes)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "    return tloss, tacc"
      ],
      "metadata": {
        "id": "8wjPz7DHqKcL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:55:44.337587Z",
          "iopub.execute_input": "2025-06-14T05:55:44.338189Z",
          "iopub.status.idle": "2025-06-14T05:55:44.344999Z",
          "shell.execute_reply.started": "2025-06-14T05:55:44.338164Z",
          "shell.execute_reply": "2025-06-14T05:55:44.344214Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ],
      "metadata": {
        "id": "Q5npQNFH315V",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:55:53.855685Z",
          "iopub.execute_input": "2025-06-14T05:55:53.855988Z",
          "iopub.status.idle": "2025-06-14T05:55:53.86304Z",
          "shell.execute_reply.started": "2025-06-14T05:55:53.855964Z",
          "shell.execute_reply": "2025-06-14T05:55:53.862135Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = os.path.join(MODEL_DIR, \"best_model.pt\")\n",
        "best_acc = -np.inf\n",
        "for epoch in range(EPOCHS):\n",
        "    ### clean up memory before computation\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = eval(model, val_loader)\n",
        "\n",
        "    print(f\"\\tTrain Acc: {train_acc*100:.2f}%\\tTrain Loss: {train_loss:.4f}\\tLR: {curr_lr:.7f}\")\n",
        "    print(f\"\\tVal   Acc: {val_acc*100:.2f}%\\tVal   Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save model at every epoch\n",
        "    epoch_model_path = os.path.join(MODEL_DIR, f\"model_at_epoch_{epoch + 1}.pt\")\n",
        "    torch.save(model.state_dict(), epoch_model_path)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Updated Best Model at: {best_model_path}\")\n",
        "\n",
        "    ### take step in adjusting the learning rate\n",
        "    scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "PV37i-CInNYw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T05:56:03.66175Z",
          "iopub.execute_input": "2025-06-14T05:56:03.662355Z",
          "iopub.status.idle": "2025-06-14T07:19:55.670214Z",
          "shell.execute_reply.started": "2025-06-14T05:56:03.66233Z",
          "shell.execute_reply": "2025-06-14T07:19:55.669321Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load best model after training\n",
        "model.load_state_dict(torch.load('/content/best_model.pt'))\n"
      ],
      "metadata": {
        "id": "Iq9ucmFYnpeC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:20:09.030594Z",
          "iopub.execute_input": "2025-06-14T07:20:09.030889Z",
          "iopub.status.idle": "2025-06-14T07:20:09.094586Z",
          "shell.execute_reply.started": "2025-06-14T07:20:09.030862Z",
          "shell.execute_reply": "2025-06-14T07:20:09.093781Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "model.eval()  # Set model in evaluation mode\n",
        "predicted = []\n",
        "groundtruth = []\n",
        "\n",
        "for frames, phonemes in val_loader:\n",
        "\n",
        "    # Move data to device\n",
        "    frames = frames.to(device)\n",
        "    phonemes = phonemes.to(device)\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.inference_mode():\n",
        "        logits = model(frames)\n",
        "\n",
        "    predict = torch.argmax(logits, dim = 1)\n",
        "\n",
        "    # Detach and move to CPU for evaluation\n",
        "    predicted.extend(predict.detach().cpu().tolist())\n",
        "    groundtruth.extend(phonemes.detach().cpu().tolist())\n",
        "\n",
        "    # Release memory\n",
        "    del frames, phonemes, logits, predict\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(\n",
        "    groundtruth,\n",
        "    predicted,\n",
        "    target_names = PHONEMES  # Skipping SOS and EOS tokens\n",
        "))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:20:12.75493Z",
          "iopub.execute_input": "2025-06-14T07:20:12.75522Z",
          "iopub.status.idle": "2025-06-14T07:21:32.351637Z",
          "shell.execute_reply.started": "2025-06-14T07:20:12.7552Z",
          "shell.execute_reply": "2025-06-14T07:21:32.350436Z"
        },
        "id": "GC_l3CLzUVKt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"darkgrid\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:21:32.353284Z",
          "iopub.execute_input": "2025-06-14T07:21:32.353531Z",
          "iopub.status.idle": "2025-06-14T07:21:32.688604Z",
          "shell.execute_reply.started": "2025-06-14T07:21:32.353506Z",
          "shell.execute_reply": "2025-06-14T07:21:32.687813Z"
        },
        "id": "7-SA6nuzUVKu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(groundtruth, predicted)\n",
        "\n",
        "# Normalize confusion matrix by row (i.e., by true labels)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis = 1, keepdims = True)\n",
        "\n",
        "# Replace NaNs (from division by zero, if any row sum is 0)\n",
        "cm_normalized = np.nan_to_num(cm_normalized)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 16))\n",
        "sns.heatmap(cm_normalized,\n",
        "            annot=True,\n",
        "            fmt=\".1f\",\n",
        "            cmap=\"Greens\",\n",
        "            xticklabels=PHONEMES,\n",
        "            yticklabels=PHONEMES,\n",
        "            cbar_kws={'label': 'Proportion'})\n",
        "\n",
        "plt.title('Normalized Confusion Matrix', fontsize=16)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:21:32.689633Z",
          "iopub.execute_input": "2025-06-14T07:21:32.689973Z",
          "iopub.status.idle": "2025-06-14T07:21:37.688338Z",
          "shell.execute_reply.started": "2025-06-14T07:21:32.689955Z",
          "shell.execute_reply": "2025-06-14T07:21:37.687586Z"
        },
        "id": "WClQn6Y1UVKv"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}