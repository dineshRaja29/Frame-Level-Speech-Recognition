{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshRaja29/Frame-Level-Speech-Recognition/blob/main/EXPERIMENT_I_WITHOUT_OUTPUT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'>Frame-Level Speech Recognition"
      ],
      "metadata": {
        "id": "F9ERgBpbcMmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'>GOAL:\n",
        "Given data which consists of melspectrograms, and phoneme labels for each 28-dimensional vector in the melspectrogram. The task is to predict the label of a particular 28-dimensional vector in an utterance (plus optional context) using a Feed Forward Deep Neural Network (FF-DNN or MLP).\n"
      ],
      "metadata": {
        "id": "CLkH6GMGcWcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'> OBJECTIVES:\n",
        "\n",
        "\n",
        "* The main goal here is to explore neural networks for speech recognition, mainly focusing on phoneme state labelling.\n",
        "* Speech recognition is an important field in deep learning with multiple applications. Speech is a fundamental form of human communication.\n",
        "* Speech recognition involves converting spoken language into written text\n",
        "or data that could be understood by machines.\n",
        "* Speech data refers to audio recordings of human speech, while phonemes represent the smallest units of sound that can convey a different meaning(bake, take: b,t).\n",
        "* Spectrograms are visual representations of the acoustic properties of speech signals, i.e. captures the changes in the frequency over time."
      ],
      "metadata": {
        "id": "4Ol8PUJ9TLkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'> DATASET DETAILS\n",
        "\n",
        "* Dataset contains audio recordings (utterances) and their phoneme state (subphoneme) labels.\n",
        "* The data comes from articles published in the Wall Street Journal (WSJ) that are read aloud and labelled using the original text.\n",
        "* The dataset provided has speech data in the form of Mel spectrograms.\n",
        "* The data comprises of:\n",
        "  * Speech recordings (raw mel spectrogram frames)\n",
        "  * Frame-level phoneme state labels\n",
        "* Training data have around 28539 samples\n",
        "\n"
      ],
      "metadata": {
        "id": "CQ6MrCLYY9Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'green'> Phonemes and Phoneme States\n",
        "* As letters are the atomic elements of written language, phonemes are the atomic elements of speech.\n",
        "* It is crucial for us to have a means to distinguish different sounds in speech that may or may not represent the same letter or combinations of letters in the written alphabet.\n",
        "* In the dataset, we will consider a total of 40 phonemes in this language.\n",
        "* A powerful technique in speech recognition is to model speech as a markov process with unobserved states.\n",
        "* This model considers observed speech to be dependent on unobserved state transitions. We refer to these unobserved states as phoneme states or subphonemes. For each phoneme, there are 3 respective phoneme\n",
        "states. The transition graph of the phoneme states for a given phoneme is as follows:\n",
        "* Example: [”+BREATH+”, ”+COUGH+”, ”+NOISE+”, ”+SMACK+”, ”+UH+”, ”+UM+”,  \"AA”, ”AE”, ”AH”, ”AO”, ”AW”, ”AY”, ”B”, ”CH”, ”D”, ”DH”, ”EH”, ”ER”, ”EY”, ”F”, ”G”, ”HH”, ”IH”, ”IY”, ”JH”, ”K”, ”L”, ”M”, ”N”, ”NG”, ”OW”, ”OY”, ”P”, ”R”, ”S”, ”SH”, ”SIL”, ”T”, ”TH”, ”UH”, ”UW”, ”V”, ”W”, ”Y”, ”Z”, ”ZH”]\n",
        "\n",
        "* Hidden Markov Models (HMMs) estimate the parameters of this unobserved markov process (transition and emission probabilities) that maximize the likelihood of the observed speech data.\n",
        "* We will take model-free approach and classify mel spectrogram frames using a neural network that takes a frame (plus optional context) and outputs class probabilities for all 40 phoneme states.\n",
        "The training data has the corresponding phonemes for this data and we need to train an MLP for predicting.\n",
        "We will be building a multilayer perceptron(MLP) that can effectively recognize and\n",
        "label the phoneme states in the training data. An MLP is a type of neural network that comprises multiple\n",
        "layers of perceptrons, that help it capture the features and patterns of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "T-jaKQbJcv-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'green'>Speech Representation\n",
        "* Raw speech signal (also known as the speech waveform) is stored simply as a sequence of numbers that\n",
        "represent the amplitude of the sound wave at each time step. This signal is typically composed of sound\n",
        "waves of several different frequencies overlaid on top of one another. For human speech, these frequencies\n",
        "represent the frequencies at which the vocal tract vibrates when we speak and produce sound. Since this\n",
        "signal is not very useful for speech recognition if used directly as a waveform, we convert it into a more useful\n",
        "representation called a \"melspectrogram\" in the feature extraction stage.\n",
        "* The variation with time of the frequencies present in a particular speech sample are very useful in determining the phoneme being spoken. In order to separate out all the individual frequencies present in the signal, we\n",
        "perform a variant of the Fourier Transform, called the Short-Time Fourier Transform (STFT) on small,\n",
        "overlapping segments (called frames, each of 25ms) of the waveform. A single vector is produced as the result\n",
        "of this transform. Since we use a stride of 10ms between each frame, we end up with 100 vectors per second of\n",
        "speech. Finally, we convert each vector into a 28-dimensional vector (for further readings https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html). For an utterance T seconds long, this leaves us with a matrix\n",
        "of shape (100\\*T, 28) known as the melspectrogram. Note that in the dataset provided to you, we have\n",
        "already done all of this pre-processing and provided the final (\\*, 28) shaped melspectrograms to you.\n",
        "The data provided consists of these melspectrograms, and phoneme labels\n",
        "for each 28-dimensional vector in the melspectrogram. The task is to predict\n",
        "the label of a particular 28-dimensional vector in an utterance."
      ],
      "metadata": {
        "id": "OFq90bkydK82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'> PERFORMANCE METRIC\n",
        "\n",
        "* Accuracy\n",
        "* Confusion Matrix\n",
        "* Precision and Recall"
      ],
      "metadata": {
        "id": "TJZubBb7vZwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'green'> EXPERIMENT\n",
        "\n",
        "* Build FF-DNN only without any batchnorm or dropout\n",
        "* Different right and left context are added so that capture more variability in the speaker utterances smoothing\n",
        "* Sparse MLP is created using the loss function\n",
        "    * Reference: https://ieeexplore.ieee.org/document/5734801\n",
        "* No Augmentation\n",
        "* No hyper-parameter tuning however used our past knowledge to set hyper-paramters\n",
        "* Advanced weight initialization, Label smoothing in loss function, gradient clipping and scheduler are used."
      ],
      "metadata": {
        "id": "sl8pUcOV2WiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /content/"
      ],
      "metadata": {
        "trusted": true,
        "id": "q96sZSfMvZwe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary --quiet"
      ],
      "metadata": {
        "id": "rwYu9sSUnSho",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import torchsummary\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "id": "qI4qfx7tiBZt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:17:21.777819Z",
          "iopub.execute_input": "2025-06-17T15:17:21.778114Z",
          "iopub.status.idle": "2025-06-17T15:17:27.26173Z",
          "shell.execute_reply.started": "2025-06-17T15:17:21.778088Z",
          "shell.execute_reply": "2025-06-17T15:17:27.260982Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # using colab for including google drive to save model checkpoints in a folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8yBgXjKV1O0Z",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir -p /root/.kaggle"
      ],
      "metadata": {
        "id": "Q90JyTKuraM1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:17:27.26348Z",
          "iopub.execute_input": "2025-06-17T15:17:27.263858Z",
          "iopub.status.idle": "2025-06-17T15:17:32.415532Z",
          "shell.execute_reply.started": "2025-06-17T15:17:27.263837Z",
          "shell.execute_reply": "2025-06-17T15:17:32.414223Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"dineshbuswala\",\"key\":\"a67fefaecacb98180c9d56e1b00b37de\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TPBUd7Cnl-Rx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:17:40.74305Z",
          "iopub.execute_input": "2025-06-17T15:17:40.74369Z",
          "iopub.status.idle": "2025-06-17T15:17:40.878997Z",
          "shell.execute_reply.started": "2025-06-17T15:17:40.743627Z",
          "shell.execute_reply": "2025-06-17T15:17:40.877884Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# commands to download data from kaggle\n",
        "\n",
        "!kaggle competitions download -c 11785-hw1p2-f23 --force\n",
        "!mkdir -p '/content/data'\n",
        "\n",
        "# !unzip -qo /content/11785-hw1p2-f23.zip -d '/content/data'"
      ],
      "metadata": {
        "id": "if2Somqfbje1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:17:43.362362Z",
          "iopub.execute_input": "2025-06-17T15:17:43.363151Z",
          "iopub.status.idle": "2025-06-17T15:18:09.752561Z",
          "shell.execute_reply.started": "2025-06-17T15:17:43.363117Z",
          "shell.execute_reply": "2025-06-17T15:18:09.751724Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo /kaggle/working/11785-hw1p2-f23.zip -d '/content/data'"
      ],
      "metadata": {
        "trusted": true,
        "id": "2ghCRGEaSN2R",
        "execution": {
          "iopub.status.busy": "2025-06-17T15:18:09.754487Z",
          "iopub.execute_input": "2025-06-17T15:18:09.754809Z",
          "iopub.status.idle": "2025-06-17T15:19:09.677012Z",
          "shell.execute_reply.started": "2025-06-17T15:18:09.754781Z",
          "shell.execute_reply": "2025-06-17T15:19:09.676158Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r /content/data/11-785-f23-hw1p2/test-clean\n",
        "! rm -r /content/11785-hw1p2-f23.zip"
      ],
      "metadata": {
        "id": "90HjJ5xEegS3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /content/data/11-785-f23-hw1p2/train-clean-100/mfcc/ | wc -l"
      ],
      "metadata": {
        "id": "32hJUYftmPDf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:19:09.678486Z",
          "iopub.execute_input": "2025-06-17T15:19:09.678733Z",
          "iopub.status.idle": "2025-06-17T15:19:09.917335Z",
          "shell.execute_reply.started": "2025-06-17T15:19:09.678709Z",
          "shell.execute_reply": "2025-06-17T15:19:09.91631Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "### configuration variables\n",
        "EPOCHS                          = 4\n",
        "BATCH_SIZE                      = 2048 * 2\n",
        "LEFT_CONTEXT                    = 7\n",
        "RIGHT_CONTEXT       \t        = 7\n",
        "INITIAL_LEARNING_RATE           = 1e-3\n",
        "L2_PENALTY                      = 1e-5\n",
        "STEP_SIZE                       = 2\n",
        "GAMMA                           = 0.1\n",
        "BASE_DIRECTORY                  = '/content/data/11-785-f23-hw1p2/'\n",
        "TRAINING_DATA                   = BASE_DIRECTORY + 'train-clean-100'\n",
        "EVALUATION_DATA                 = BASE_DIRECTORY + 'dev-clean'\n",
        "PHONEMES                        = ['[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "                                    'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "                                    'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "                                    'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "                                    'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "                                    'V',     'W',     'Y',     'Z',     'ZH'] #,    '[SOS]', '[EOS]']\n",
        "PHONEMES_TO_INDEX                = {phoneme: idx for idx, phoneme in enumerate(PHONEMES)}\n",
        "NUMBER_OF_NEURONS                = [2048, 2048, 1024, 1024, 512, 256, 256]\n",
        "MODEL_DIR                        = \"/content\"\n",
        "CLIP_VALUE                       = 1.0\n",
        "LABEL_SMOOTHING                  = 0.01"
      ],
      "metadata": {
        "id": "Kc4IbXuwx0Tf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:19:13.232202Z",
          "iopub.execute_input": "2025-06-17T15:19:13.232767Z",
          "iopub.status.idle": "2025-06-17T15:19:14.115885Z",
          "shell.execute_reply.started": "2025-06-17T15:19:13.232733Z",
          "shell.execute_reply": "2025-06-17T15:19:14.115054Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, phonemes = PHONEMES_TO_INDEX,\n",
        "                 left_context = LEFT_CONTEXT,\n",
        "                 right_context = RIGHT_CONTEXT):\n",
        "\n",
        "        self.left_context = left_context\n",
        "        self.right_context = right_context\n",
        "        self.phonemes_mapping = phonemes\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, 'mfcc')\n",
        "        self.transcript_dir = os.path.join(root, 'transcript')\n",
        "\n",
        "        # List and sort mfcc and transcript files\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "        transcript_names = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(mfcc_names) == len(transcript_names), \"Mismatch in number of MFCC and transcript files\"\n",
        "\n",
        "        total_frames = 0\n",
        "\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc_path = os.path.join(self.mfcc_dir, mfcc_names[i])\n",
        "\n",
        "            mfcc = np.load(mfcc_path,\n",
        "                           allow_pickle = False,\n",
        "                           mmap_mode='r')\n",
        "            total_frames += mfcc.shape[0]\n",
        "            del mfcc\n",
        "\n",
        "        sample_mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[0]), allow_pickle=False)\n",
        "        self.mfcc_dim = sample_mfcc.shape[1]\n",
        "        ### Right Padding is added here automatically\n",
        "        self.mfccs = np.zeros((total_frames + right_context, self.mfcc_dim))\n",
        "        self.transcripts = [None] * (total_frames + right_context)\n",
        "        ### Release memory\n",
        "        del sample_mfcc, total_frames\n",
        "        gc.collect()\n",
        "\n",
        "        index = 0\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]),\n",
        "                           allow_pickle = False,\n",
        "                           mmap_mode = 'r')\n",
        "\n",
        "            mfcc = (mfcc - mfcc.mean(axis = 1, keepdims = True)) / (mfcc.std(axis = 1, keepdims = True) + 1e-5)\n",
        "\n",
        "            transcript = np.load(os.path.join(self.transcript_dir, transcript_names[i]),\n",
        "                                 allow_pickle = False,\n",
        "                                 mmap_mode='r')[1:-1]\n",
        "\n",
        "            self.mfccs[index: index + mfcc.shape[0]] = mfcc\n",
        "            self.transcripts[index: index + len(transcript)] = transcript\n",
        "            index += mfcc.shape[0]\n",
        "            del mfcc, transcript\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                gc.collect() ### Release memory\n",
        "\n",
        "        # Save original dataset length (before adding right padding)\n",
        "        self.length = len(self.mfccs) - right_context\n",
        "\n",
        "        # Map transcript phonemes to indices\n",
        "        self.transcripts = [self.phonemes_mapping.get(p, -1) for p in self.transcripts]\n",
        "\n",
        "        ### Release memory\n",
        "        del mfcc_names, transcript_names\n",
        "        gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        if ind < self.left_context:  # index is less than left context\n",
        "            # zeros need to prepend with it\n",
        "            padding = []\n",
        "            for _ in range(self.left_context - ind):\n",
        "                padding.append(np.zeros((1, self.mfcc_dim)))\n",
        "\n",
        "            for i in range(ind):\n",
        "                padding.append(self.mfccs[i][None, :])\n",
        "\n",
        "            # Include current and right context frames\n",
        "            current = self.mfccs[ind][None, :]  # Ensure shape (1, 28)\n",
        "            right_context = [self.mfccs[i][None, :] for i in range(ind + 1, ind + 1 + self.right_context)]\n",
        "            frames = np.concatenate(padding + [current] + right_context, axis=0)\n",
        "\n",
        "        else:  # when index is greater than or equal to left context\n",
        "            left_context_frames = self.mfccs[ind - self.left_context: ind]\n",
        "            current = self.mfccs[ind][None, :]  # Ensure shape (1, 28)\n",
        "            right_context_frames = self.mfccs[ind + 1: ind + 1 + self.right_context]\n",
        "\n",
        "            frames = np.concatenate([left_context_frames, current, right_context_frames], axis=0)\n",
        "\n",
        "        frames = frames.flatten()  # Flatten to get 1D data\n",
        "        frames = torch.FloatTensor(frames)  # Convert to tensor\n",
        "        phonemes = torch.tensor(self.transcripts[ind], dtype=torch.long)  # Convert label to tensor\n",
        "\n",
        "        return frames, phonemes\n"
      ],
      "metadata": {
        "id": "gUN5oYyqq8ji",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:19:17.262478Z",
          "iopub.execute_input": "2025-06-17T15:19:17.263679Z",
          "iopub.status.idle": "2025-06-17T15:19:17.281451Z",
          "shell.execute_reply.started": "2025-06-17T15:19:17.263612Z",
          "shell.execute_reply": "2025-06-17T15:19:17.280529Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object using the AudioDataset class for the training data\n",
        "train_data = AudioDataset(TRAINING_DATA)\n",
        "\n",
        "# Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = AudioDataset(EVALUATION_DATA)\n"
      ],
      "metadata": {
        "id": "7xi7V8x8W9z4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:19:22.857202Z",
          "iopub.execute_input": "2025-06-17T15:19:22.857486Z",
          "iopub.status.idle": "2025-06-17T15:20:54.330645Z",
          "shell.execute_reply.started": "2025-06-17T15:19:22.857468Z",
          "shell.execute_reply": "2025-06-17T15:20:54.330022Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ],
      "metadata": {
        "id": "4mzoYfTKu14s",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:20:54.33188Z",
          "iopub.execute_input": "2025-06-17T15:20:54.332151Z",
          "iopub.status.idle": "2025-06-17T15:20:54.338059Z",
          "shell.execute_reply.started": "2025-06-17T15:20:54.33212Z",
          "shell.execute_reply": "2025-06-17T15:20:54.337168Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing code to check if data loaders are working as expecting\n",
        "# total_batches = len(train_loader)\n",
        "\n",
        "# for i, (frames, phoneme) in enumerate(train_loader):\n",
        "#     if i < 10 or i >= total_batches - 10:\n",
        "#         print(f\"Batch {i + 1}/{total_batches}\")\n",
        "#         print(\"Frames:\", frames)\n",
        "#         print(\"Phoneme:\", phoneme)\n",
        "#         print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "n-GV3UvgLSoF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Neurons in each layer: input -> hidden(s) -> output\n",
        "        self.neurons = [input_size] + NUMBER_OF_NEURONS + [len(PHONEMES)]\n",
        "\n",
        "        layers = []\n",
        "        for in_features, out_features in zip(self.neurons[:-2], self.neurons[1:-1]):\n",
        "            layers.append(torch.nn.Linear(in_features, out_features))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "\n",
        "        # Final layer (no activation)\n",
        "        layers.append(torch.nn.Linear(self.neurons[-2], self.neurons[-1]))\n",
        "\n",
        "        # Combine all into a sequential model\n",
        "        self.model = torch.nn.Sequential(*layers)\n",
        "\n",
        "        # Apply weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        print('Initialization of weights using Kaiming')\n",
        "        for m in self.model:\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                # Kaiming initialization for weights\n",
        "                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "OvcpontXQq9j",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:00.982358Z",
          "iopub.execute_input": "2025-06-17T15:21:00.982679Z",
          "iopub.status.idle": "2025-06-17T15:21:00.989985Z",
          "shell.execute_reply.started": "2025-06-17T15:21:00.982658Z",
          "shell.execute_reply": "2025-06-17T15:21:00.988989Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = (LEFT_CONTEXT + RIGHT_CONTEXT + 1) * 28\n",
        "model = Network(INPUT_SIZE).to(device)\n",
        "# Pass the input size as a tuple, without the batch dimension\n",
        "torchsummary.summary(model, (INPUT_SIZE,))"
      ],
      "metadata": {
        "id": "d8HWyYl82Pnc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:04.267127Z",
          "iopub.execute_input": "2025-06-17T15:21:04.267826Z",
          "iopub.status.idle": "2025-06-17T15:21:04.842852Z",
          "shell.execute_reply.started": "2025-06-17T15:21:04.2678Z",
          "shell.execute_reply": "2025-06-17T15:21:04.842142Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseLoss(torch.nn.Module):\n",
        "    def __init__(self, model, lambda_l1=1e-4):\n",
        "        super(SparseLoss, self).__init__()\n",
        "        self.ce_criterion = torch.nn.CrossEntropyLoss(label_smoothing = LABEL_SMOOTHING)\n",
        "        self.lambda_l1 = lambda_l1\n",
        "        self.model = model\n",
        "\n",
        "        # Collect indices of Linear layers, skipping first and last\n",
        "        self.linear_layer_indices = [\n",
        "            i for i, layer in enumerate(self.model.model[1:-1], 1)\n",
        "            if isinstance(layer, torch.nn.Linear)\n",
        "        ]\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce_loss = self.ce_criterion(logits, targets)\n",
        "\n",
        "        sparse_loss = 0.0\n",
        "        for i in self.linear_layer_indices:\n",
        "            weight = self.model.model[i].weight  # [out_features, in_features]\n",
        "\n",
        "            # Neuron-wise aggregation: sum inputs per neuron\n",
        "            neuron_weights = weight.sum(dim=1)  # [out_features]\n",
        "\n",
        "            # sum(log(1 + (neuron)^2))\n",
        "            sparse_loss += torch.log1p(neuron_weights.pow(2)).sum()  # log1p(x) = log(1 + x)\n",
        "\n",
        "        total_loss = ce_loss + self.lambda_l1 * sparse_loss\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:19.04227Z",
          "iopub.execute_input": "2025-06-17T15:21:19.042912Z",
          "iopub.status.idle": "2025-06-17T15:21:19.049748Z",
          "shell.execute_reply.started": "2025-06-17T15:21:19.042887Z",
          "shell.execute_reply": "2025-06-17T15:21:19.048851Z"
        },
        "id": "ikpQCB8bvZws"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# criterion = torch.nn.CrossEntropyLoss(label_smoothing = LABEL_SMOOTHING) # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "criterion = SparseLoss(model, lambda_l1 = 1e-4)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr = INITIAL_LEARNING_RATE,\n",
        "                             weight_decay = L2_PENALTY) #Defining Optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size = STEP_SIZE, gamma = GAMMA\n",
        ")\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
      ],
      "metadata": {
        "id": "UROGEVJevKD-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:22.696817Z",
          "iopub.execute_input": "2025-06-17T15:21:22.697115Z",
          "iopub.status.idle": "2025-06-17T15:21:26.181359Z",
          "shell.execute_reply.started": "2025-06-17T15:21:22.697095Z",
          "shell.execute_reply": "2025-06-17T15:21:26.180441Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XblOHEVtKab2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:32.82697Z",
          "iopub.execute_input": "2025-06-17T15:21:32.827871Z",
          "iopub.status.idle": "2025-06-17T15:21:33.130414Z",
          "shell.execute_reply.started": "2025-06-17T15:21:32.827844Z",
          "shell.execute_reply": "2025-06-17T15:21:33.129767Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        logits  = model(frames)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss    = criterion(logits, phonemes)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "    return tloss, tacc"
      ],
      "metadata": {
        "id": "8wjPz7DHqKcL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:38.836963Z",
          "iopub.execute_input": "2025-06-17T15:21:38.837739Z",
          "iopub.status.idle": "2025-06-17T15:21:38.845023Z",
          "shell.execute_reply.started": "2025-06-17T15:21:38.837714Z",
          "shell.execute_reply": "2025-06-17T15:21:38.844071Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ],
      "metadata": {
        "id": "Q5npQNFH315V",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:47.207108Z",
          "iopub.execute_input": "2025-06-17T15:21:47.207923Z",
          "iopub.status.idle": "2025-06-17T15:21:47.214134Z",
          "shell.execute_reply.started": "2025-06-17T15:21:47.207893Z",
          "shell.execute_reply": "2025-06-17T15:21:47.213327Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = os.path.join(MODEL_DIR, \"best_model.pt\")\n",
        "best_acc = -np.inf\n",
        "for epoch in range(EPOCHS):\n",
        "    ### clean up memory before computation\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = eval(model, val_loader)\n",
        "\n",
        "    print(f\"\\tTrain Acc: {train_acc*100:.2f}%\\tTrain Loss: {train_loss:.4f}\\tLR: {curr_lr:.7f}\")\n",
        "    print(f\"\\tVal   Acc: {val_acc*100:.2f}%\\tVal   Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save model at every epoch\n",
        "    epoch_model_path = os.path.join(MODEL_DIR, f\"model_at_epoch_{epoch + 1}.pt\")\n",
        "    torch.save(model.state_dict(), epoch_model_path)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Updated Best Model at: {best_model_path}\")\n",
        "\n",
        "    ### take step in adjusting the learning rate\n",
        "    scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "PV37i-CInNYw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T15:21:51.396877Z",
          "iopub.execute_input": "2025-06-17T15:21:51.397599Z",
          "iopub.status.idle": "2025-06-17T16:28:26.753769Z",
          "shell.execute_reply.started": "2025-06-17T15:21:51.397576Z",
          "shell.execute_reply": "2025-06-17T16:28:26.752813Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T16:30:23.144275Z",
          "iopub.execute_input": "2025-06-17T16:30:23.144594Z",
          "iopub.status.idle": "2025-06-17T16:30:23.494403Z",
          "shell.execute_reply.started": "2025-06-17T16:30:23.144566Z",
          "shell.execute_reply": "2025-06-17T16:30:23.493655Z"
        },
        "id": "NSd4XrgKvZwv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model after training\n",
        "model.load_state_dict(torch.load('/content/best_model.pt'))"
      ],
      "metadata": {
        "id": "Iq9ucmFYnpeC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T16:30:28.148431Z",
          "iopub.execute_input": "2025-06-17T16:30:28.149391Z",
          "iopub.status.idle": "2025-06-17T16:30:28.183568Z",
          "shell.execute_reply.started": "2025-06-17T16:30:28.149358Z",
          "shell.execute_reply": "2025-06-17T16:30:28.182698Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "model.eval()  # Set model in evaluation mode\n",
        "predicted = []\n",
        "groundtruth = []\n",
        "\n",
        "for frames, phonemes in val_loader:\n",
        "\n",
        "    # Move data to device\n",
        "    frames = frames.to(device)\n",
        "    phonemes = phonemes.to(device)\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.inference_mode():\n",
        "        logits = model(frames)\n",
        "\n",
        "    predict = torch.argmax(logits, dim = 1)\n",
        "\n",
        "    # Detach and move to CPU for evaluation\n",
        "    predicted.extend(predict.detach().cpu().tolist())\n",
        "    groundtruth.extend(phonemes.detach().cpu().tolist())\n",
        "\n",
        "    # Release memory\n",
        "    del frames, phonemes, logits, predict\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(\n",
        "    groundtruth,\n",
        "    predicted,\n",
        "    target_names = PHONEMES  # Skipping SOS and EOS tokens\n",
        "))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T16:30:31.603947Z",
          "iopub.execute_input": "2025-06-17T16:30:31.604775Z",
          "iopub.status.idle": "2025-06-17T16:31:52.940295Z",
          "shell.execute_reply.started": "2025-06-17T16:30:31.604748Z",
          "shell.execute_reply": "2025-06-17T16:31:52.939204Z"
        },
        "id": "SJx--pnrvZwx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"darkgrid\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T16:31:52.942062Z",
          "iopub.execute_input": "2025-06-17T16:31:52.942406Z",
          "iopub.status.idle": "2025-06-17T16:31:53.192339Z",
          "shell.execute_reply.started": "2025-06-17T16:31:52.942381Z",
          "shell.execute_reply": "2025-06-17T16:31:53.191781Z"
        },
        "id": "t8M7y71YvZwx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(groundtruth, predicted)\n",
        "\n",
        "# Normalize confusion matrix by row (i.e., by true labels)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis = 1, keepdims = True)\n",
        "\n",
        "# Replace NaNs (from division by zero, if any row sum is 0)\n",
        "cm_normalized = np.nan_to_num(cm_normalized)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 16))\n",
        "sns.heatmap(cm_normalized,\n",
        "            annot=True,\n",
        "            fmt=\".1f\",\n",
        "            cmap=\"Greens\",\n",
        "            xticklabels=PHONEMES,\n",
        "            yticklabels=PHONEMES,\n",
        "            cbar_kws={'label': 'Proportion'})\n",
        "\n",
        "plt.title('Normalized Confusion Matrix', fontsize=16)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-17T16:31:53.193008Z",
          "iopub.execute_input": "2025-06-17T16:31:53.193304Z",
          "iopub.status.idle": "2025-06-17T16:31:58.26957Z",
          "shell.execute_reply.started": "2025-06-17T16:31:53.193287Z",
          "shell.execute_reply": "2025-06-17T16:31:58.268703Z"
        },
        "id": "FnCbyYHuvZwx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fiqYBZNHvZwy"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}